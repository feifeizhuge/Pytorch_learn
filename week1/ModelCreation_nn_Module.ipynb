{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 学习内容概述\n",
    "\n",
    "<br/> \n",
    "回顾机器学习流程图，之前我们已经学习了，如何从硬盘读入数据，然后经过数据预处理模块，对数据进行裁剪，增强等一些列操作后，得到张量，接下来就是要把这些张量输入到神经网络模型中去，进行一系列复杂的运算处理，从而完成**目标检测，分割**等操作。\n",
    "<br/> \n",
    "<br/> \n",
    "<img src=\"picture/机器学习训练步骤.png\">\n",
    "\n",
    "<br/> \n",
    "本节介绍网络模型的基本类nn.Module，nn.Module是所有网络层的基本类，它拥有8个有序字典，用于管理模型属性，本节课中将要学习如何构建一个Module。\n",
    "\n",
    "然后通过网络结构和计算图两个角度去观察搭建一个网络模型需要两个步骤：**第一步，搭建子模块**；**第二步，拼接子模块**。\n",
    "\n",
    "----\n",
    "\n",
    "# 2. 网络模型创建步骤\n",
    "\n",
    "<br/> \n",
    "模型的创建，分为 **构建网络层** 和 **拼接网络层** ，其中构建网络层就是构建子模块:比如卷积层，池化层，激活层等，接下来就是将这些子模块按照一定的拓扑结构拼接成网络，从而形成我们所熟知的一些复杂的网络，比如:Lenet, ResNet    \n",
    "\n",
    "接下来就是模型的初始化，已经有学者提出的方法:Xavier, Kaiming, 均匀分布等    \n",
    "\n",
    "这些全都包含在pytorch的 nn.Module模块中    \n",
    "\n",
    "<img src=\"picture/nn.Module框图.png\" width=550>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Lenet实例分析\n",
    "\n",
    "<br/> \n",
    "在经典的卷积神经网络的结构中，分为如下子模块: 卷积层1，池化层1，卷积层2，池化层2，全连阶层1-2-3，共计7层结构。从输入的$32*32*3$，到最后输出一个长度为10的概率值向量。\n",
    "<img src=\"picture/lenet01.png\" width=750>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在此设置断点，step into\n",
    "net = LeNet(classes=2)\n",
    "\n",
    "# 跳转到此处\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self, classes):\n",
    "------->super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时输入self，会得到:\n",
    "```python\n",
    "self\n",
    "*** AttributeError: 'LeNet' object has no attribute '_modules'\n",
    "```\n",
    "在顺序执行所有的单行代码，得到:\n",
    "```python\n",
    "self\n",
    "LeNet(\n",
    "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
    "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
    "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
    "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
    ")\n",
    "--Return--\n",
    "```\n",
    "此时完成了，子模块类的**构建**。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ step 2/5 模型 ============================\n",
    "\n",
    "net = LeNet(classes=2)\n",
    "net.initialize_weights()\n",
    "\n",
    "# ============================ step 3/5 损失函数 ============================\n",
    "criterion = nn.CrossEntropyLoss()                                                   # 选择损失函数\n",
    "\n",
    "# ============================ step 4/5 优化器 ============================\n",
    "optimizer = optim.SGD(net.parameters(), lr=LR, momentum=0.9)                        # 选择优化器\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)     # 设置学习率下降策略\n",
    "\n",
    "# ============================ step 5/5 训练 ============================\n",
    "train_curve = list()\n",
    "valid_curve = list()\n",
    "\n",
    "for epoch in range(MAX_EPOCH):\n",
    "\n",
    "    loss_mean = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    net.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "\n",
    "        # forward\n",
    "        inputs, labels = data\n",
    "------->outputs = net(inputs)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # 统计分类情况\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).squeeze().sum().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里完成了，net类的实例化，载入了Input，在这里step into会跳转到`module.p`的`__call__()函数`\n",
    "```python\n",
    " def __call__(self, *input, **kwargs):\n",
    "        for hook in self._forward_pre_hooks.values():\n",
    "            result = hook(self, input)\n",
    "            if result is not None:\n",
    "                if not isinstance(result, tuple):\n",
    "                    result = (result,)\n",
    "                input = result\n",
    "        if torch._C._get_tracing_state():\n",
    "            result = self._slow_forward(*input, **kwargs)\n",
    "        else:\n",
    "------------>result = self.forward(*input, **kwargs)\n",
    "```\n",
    "在进入到`result = self.forward(*input, **kwargs)`，就会跳转到自己定义的`forward函数`\n",
    "```python\n",
    "def forward(self, x):\n",
    "    out = F.relu(self.conv1(x))\n",
    "    out = F.max_pool2d(out, 2)\n",
    "    out = F.relu(self.conv2(out))\n",
    "    out = F.max_pool2d(out, 2)\n",
    "    out = out.view(out.size(0), -1)\n",
    "    out = F.relu(self.fc1(out))\n",
    "    out = F.relu(self.fc2(out))\n",
    "    out = self.fc3(out)\n",
    "    return out\n",
    "```\n",
    "     \n",
    "</br> \n",
    "这里完成了网络的前向传播，也就是子模块间的**拼接**。\n",
    "\n",
    "<img src=\"picture/模块拼接.png\" width=500>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. torch.nn 中 nn.Module学习\n",
    "`torch.nn`是框架的神经网络模块，其中包含四个大类:\n",
    "\n",
    "- nn.Paramter: 它是张量的子类，表示可以学习的参数，如weight，bias\n",
    "- nn.Module: 所有网络的基类，比如Lenet是一个nn.Module类，其中的卷积层，池化层也是一个nn.Module类\n",
    "- nn.functional: 函数的具体实现，比如卷积函数，池化函数\n",
    "- nn.init: 提供了丰富的参数初始化方法\n",
    "\n",
    "<img src=\"picture/torch.nn分类.png\" width=650>\n",
    "\n",
    "---\n",
    "## 3.1 nn.Module的八个字典\n",
    "接下来具体学习其中的**nn.Module**, nn.Module包含八个重要的有序字典，也称为属性。\n",
    "```python\n",
    "self._parameters = OrderedDict()\n",
    "self._buffers = OrderedDict() \n",
    "self._backward_hooks = OrderedDict() \n",
    "self._forward_hooks = OrderedDict() \n",
    "self._forward_pre_hooks = OrderedDict() \n",
    "self._state_dict_hooks = OrderedDict() \n",
    "self._load_state_dict_pre_hooks = OrderedDict() \n",
    "self._modules = OrderedDict()\n",
    "```\n",
    "\n",
    "其中最重要的是:\n",
    "- <font color=blue>parameters: 存储管理nn.Parameter类</font>\n",
    "- <font color=blue>modules : 存储管理nn.Module类, 比如Lenet中的卷积，池化都属于这个类</font>\n",
    "- buffers:存储管理缓冲属性，如BN层中的running_mean \n",
    "- xxx_hooks:存储管理钩子函数\n",
    "\n",
    "接下来继续通过代码，理解nn.Module的创建过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Lenet 继承 nn.Module, 所以Lenet是nn.Module类\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self, classes):\n",
    "        # step into 到继承父类的函数__init()__中\n",
    "------->super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, classes)\n",
    "```\n",
    "---\n",
    "step into `__init__(self)`, 发现通过一个结构体初始化，在结构体中初始化之前提到的八个字典，着重关注`_self._module`和`self._parameter`\n",
    "```python\n",
    "def __init__(self):\n",
    "    self._construct()\n",
    "    # initialize self.training separately from the rest of the internal\n",
    "    # state, as it is managed differently by nn.Module and ScriptModule\n",
    "    self.training = True\n",
    "\n",
    "def _construct(self):\n",
    "    \"\"\"\n",
    "    Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
    "    \"\"\"\n",
    "    torch._C._log_api_usage_once(\"python.nn_module\")\n",
    "    self._backend = thnn_backend\n",
    "    self._parameters = OrderedDict()\n",
    "    self._buffers = OrderedDict()\n",
    "    self._backward_hooks = OrderedDict()\n",
    "    self._forward_hooks = OrderedDict()\n",
    "    self._forward_pre_hooks = OrderedDict()\n",
    "    self._state_dict_hooks = OrderedDict()\n",
    "    self._load_state_dict_pre_hooks = OrderedDict()\n",
    "    self._modules = OrderedDict()\n",
    "```\n",
    "\n",
    "--- \n",
    "\n",
    "运到后return并离开`__init__()`，来到下一行:\n",
    "```python\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self, classes):\n",
    "        # step into 到继承父类的函数__init()__中\n",
    "        super(LeNet, self).__init__()\n",
    "------->self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, classes)\n",
    "```\n",
    "step into `nn.Conv2d`, 发现`Conv2d(_ConvNd)`继承`class ConvNd`,`class ConvNd`也继承`module`, 所有大家都有八个字典的属性。\n",
    "```python\n",
    "class Conv2d(_ConvNd):\n",
    "        def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1,\n",
    "                 bias=True, padding_mode='zeros'):\n",
    "        kernel_size = _pair(kernel_size)\n",
    "        stride = _pair(stride)\n",
    "        padding = _pair(padding)\n",
    "        dilation = _pair(dilation)\n",
    "        super(Conv2d, self).__init__(\n",
    "            in_channels, out_channels, kernel_size, stride, padding, dilation,\n",
    "            False, _pair(0), groups, bias, padding_mode)\n",
    "\n",
    "```\n",
    "跳转出来，回到Lenet类中，此时查看类的字典:\n",
    "```python\n",
    "# 1->调用指令\n",
    "self.__dict__\n",
    "# 得到\n",
    "{'_backend': <torch.nn.backends.thnn.THNNFunctionBackend object at 0x125bb1438>, \n",
    " '_parameters': OrderedDict(), \n",
    " '_buffers': OrderedDict(), \n",
    " '_backward_hooks': OrderedDict(), \n",
    " '_forward_hooks': OrderedDict(), \n",
    " '_forward_pre_hooks': OrderedDict(), \n",
    " '_state_dict_hooks': OrderedDict(), \n",
    " '_load_state_dict_pre_hooks': OrderedDict(), \n",
    " '_modules': OrderedDict([('conv1', Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1)))]), \n",
    " 'training': True}\n",
    "\n",
    "# 2->调用指令\n",
    "self._modules\n",
    "# 得到有序字典，key值是'conv1',value是Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1)))\n",
    "OrderedDict([('conv1', Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1)))])\n",
    "\n",
    "# 3->self是个nn.Module类，其中的卷积层也是nn.Module类, 那么它也有相对应的8个字典属性\n",
    "# 且注意到⚠️，conv1类中的'_modules'没有参数，这是因为没有子类了，卷积层是底层的类了\n",
    "self._modules['conv1'].__dict__\n",
    "# 得到\n",
    "{'_backend': <torch.nn.backends.thnn.THNNFunctionBackend object at 0x125bb1438>, \n",
    " '_parameters': OrderedDict([\n",
    "     ('weight', Parameter containing:\n",
    "     tensor([[[[ 0.0595, -0.0510, -0.0224,  0.0542, -0.1087],\n",
    "              [ 0.0692, -0.0238,  0.0587,  0.0161, -0.0141],\n",
    "              [ 0.0320,  0.0057,  0.0422, -0.0450, -0.0084],\n",
    "              [-0.0104,  0.0167, -0.0005,  0.1009,  0.0359],\n",
    "              [-0.0430, -0.0697, -0.0194, -0.0498, -0.0370]], ......blabla....\n",
    "    ], requires_grad=True, \n",
    "    ('bias', Parameter containing:\n",
    "    tensor([ 0.0387,  0.0632,  0.1145, -0.0121,  0.1111,  0.0742],requires_grad=True))]), \n",
    " '_buffers': OrderedDict(), \n",
    " '_backward_hooks': OrderedDict(), \n",
    " '_forward_hooks': OrderedDict(), \n",
    " '_forward_pre_hooks': OrderedDict(), \n",
    " '_state_dict_hooks': OrderedDict(), \n",
    " '_load_state_dict_pre_hooks': OrderedDict(), \n",
    " '_modules': OrderedDict(), \n",
    " 'training': True, \n",
    " 'in_channels': 3, \n",
    " 'out_channels': 6, \n",
    " 'kernel_size': (5, 5), \n",
    " 'stride': (1, 1), \n",
    " 'padding': (0, 0), \n",
    " 'dilation': (1, 1), \n",
    " 'transposed': False, \n",
    " 'output_padding': (0, 0), \n",
    " 'groups': 1, \n",
    " 'padding_mode': 'zeros'}\n",
    "\n",
    "# 4->卷积层类中的parameters字典中的weight是一个Tensor类，它有tensor的一些属性: _dtype, _data, _device\n",
    "self._modules['conv1']._parameters['weight'].dtype\n",
    "# 得到\n",
    "torch.float32\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.Linear(16*5*5, 120`产生一个类，再赋值给`self.fc`之前必定会进入一个函数中先<font color=red>判定</font>:\n",
    "```python\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self, classes):\n",
    "        # step into 到继承父类的函数__init()__中\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "------->self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, classes)\n",
    "```\n",
    "step into `__setattr__(self, name, value)`函数中, 这个函数会检查输入的value的属性(nn._parameters/nn._Modules/nn._buffers?)并赋值, 比如`nn.Conv2d(6, 16, 5)`属于`nn.modules类`, 不属于`nn.parameters类`, 因此`nn.Conv2d(6, 16, 5)`会存放在`Lenet`的`nn.modules`字典中保存。\n",
    "```python\n",
    "def __setattr__(self, name, value):\n",
    "        def remove_from(*dicts):\n",
    "            for d in dicts:\n",
    "                if name in d:\n",
    "                    del d[name]\n",
    "\n",
    "        params = self.__dict__.get('_parameters')\n",
    "        # 检查是否是实例\n",
    "------->if isinstance(value, Parameter):\n",
    "            if params is None:\n",
    "                raise AttributeError(\n",
    "                    \"cannot assign parameters before Module.__init__() call\")\n",
    "            remove_from(self.__dict__, self._buffers, self._modules)\n",
    "            self.register_parameter(name, value)\n",
    "        # 检查是否有parameters\n",
    "------->elif params is not None and name in params:\n",
    "            if value is not None:\n",
    "                raise TypeError(\"cannot assign '{}' as parameter '{}' \"\n",
    "                                \"(torch.nn.Parameter or None expected)\"\n",
    "                                .format(torch.typename(value), name))\n",
    "            self.register_parameter(name, value)\n",
    "        # 检查是否是Modules\n",
    "------->else:\n",
    "            modules = self.__dict__.get('_modules')\n",
    "            if isinstance(value, Module):\n",
    "                if modules is None:\n",
    "                    raise AttributeError(\n",
    "                        \"cannot assign module before Module.__init__() call\")\n",
    "                remove_from(self.__dict__, self._parameters, self._buffers)\n",
    "                # 是Modules，把value(也就是卷积层)赋值给modules['conv2']\n",
    "--------------->modules[name] = value\n",
    "            elif modules is not None and name in modules:\n",
    "                if value is not None:\n",
    "                    raise TypeError(\"cannot assign '{}' as child module '{}' \"\n",
    "                                    \"(torch.nn.Module or None expected)\"\n",
    "                                    .format(torch.typename(value), name))\n",
    "                modules[name] = value\n",
    "            else:\n",
    "                buffers = self.__dict__.get('_buffers')\n",
    "                if buffers is not None and name in buffers:\n",
    "                    if value is not None and not isinstance(value, torch.Tensor):\n",
    "                        raise TypeError(\"cannot assign '{}' as buffer '{}' \"\n",
    "                                        \"(torch.Tensor or None expected)\"\n",
    "                                        .format(torch.typename(value), name))\n",
    "                    buffers[name] = value\n",
    "                else:\n",
    "                    object.__setattr__(self, name, value)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 总结\n",
    "nn.Module总结\n",
    "- 一个module可以包含多个子module,比如LeNet是一个module,组成他的卷积层也是一个module.\n",
    "- 一个module相当于一个运算，必须实现forward()函数\n",
    "- 每个module都有8个字典管理它的属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
